
# Attention-based Neural Machine Translation

This project was completed as a part of the Honors portion of the [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models) Course on [Coursera](https://www.coursera.org/).

Credit to DeepLearning.AI and the Coursera platform for providing the course materials and guidance.

## Objective

In this task, my objective is to construct a Neural Machine Translation (NMT) model capable of converting human-readable dates ("25th of June, 2009") into machine-readable dates ("2009-06-25"). To accomplish this, I will implement an attention model, a sophisticated sequence-to-sequence model that significantly improves translation accuracy and performance. By building this attention-based NMT model, I aim to facilitate seamless and accurate translation between human-readable and machine-readable date formats. The attention mechanism will allow the model to focus on the most relevant parts of the input sequence during translation, enhancing its capability to handle complex date representations effectively. Through this assignment, I will gain a deeper understanding of NMT and the attention mechanism's contribution to improving translation tasks, making the model a valuable tool for date conversion and potentially other sequence-to-sequence applications.
## Results

![Attention-based Neural Machine Translation](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBS36aTdTVpXvyu6vU5e_wNWYe0hA4BwqW-TcWcsdEkUZIeuCcx7UqmzQyZhiEnPIKLjSVGfPzfeVJD3l8enFuHzNakMkHT9YBiRi_x-URtFrAoVx8O6p7MXhBk3tgO7Wa8hGynELjMHTdO6374K4hmHRxiYHAzNczcsAJ-eTgd6c50Us1W8CQf920ers/s1600/attention-based-neural-machine-translation.png)